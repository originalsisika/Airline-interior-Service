{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "AIS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CrSyv2vetF7"
      },
      "source": [
        "# Hackathon: Arlines interior Services"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSb56rxtetGB"
      },
      "source": [
        "- **Lihan Fang**\n",
        "- **Wen Tian**\n",
        "- **Haofan Chen**\n",
        "- **Siying Chen**\n",
        "- **Xiaotong Zhang**\n",
        "\n",
        "***Feb. 2021***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izuuYGqTetGC"
      },
      "source": [
        "# Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8ro50e1etGC"
      },
      "source": [
        "import os\n",
        "import string\n",
        "import datetime\n",
        "import time\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "!pip install gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.ldamulticore import LdaMulticore\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "!pip install aspect-based-sentiment-analysis\n",
        "import aspect_based_sentiment_analysis as absa\n",
        "\n",
        "!pip install wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgN-5Wm_etGD"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gU2v8GdnetGE"
      },
      "source": [
        "reviews = pd.read_csv('./drive/MyDrive/data/reviews_airline.csv')\n",
        "# reviews = pd.read_excel('raw_messages_for_test.xlsx')\n",
        "# reviews.date.apply(lambda x: datetime.datetime.strptime(x,\"%Y-%m-%d\"))\n",
        "reviews.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Xi-bjHveetGE"
      },
      "source": [
        "# Group reviews by airlines\n",
        "n_reviews = reviews.groupby('airline')['body'].agg('count')\n",
        "n_reviews.sort_values(inplace=True, ascending=False)\n",
        "n_reviews.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWUBm6WHetGE"
      },
      "source": [
        "# Topic Modeling\n",
        "## Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHsX9GiEetGE"
      },
      "source": [
        "# Punctuation\n",
        "punc = string.punctuation\n",
        "# Stop words\n",
        "nltk.download('stopwords')\n",
        "stop = set(stopwords.words('english'))\n",
        "# Lemma/Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "# Clean function\n",
        "def clean_doc(doc):\n",
        "    # Remove Punctuations and uncase the words\n",
        "    doc = ''.join([c.lower() for c in doc if c not in punc])\n",
        "    # Remove stop words\n",
        "    doc = ' '.join([c for c in doc.split() if c not in stop])\n",
        "    # Stemming/Lemmatizing\n",
        "    doc = ' '.join([stemmer.stem(c) for c in doc.split()])\n",
        "    # doc = ' '.join([lemma.lemmatize(c) for c in doc.split()])\n",
        "    return doc\n",
        "\n",
        "# Test the clean function\n",
        "doc = reviews[reviews.airline=='american-airlines'].body.iloc[1]\n",
        "# doc = reviews[0]\n",
        "clean_doc(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ViAuI5TetGF"
      },
      "source": [
        "# Clean the text data\n",
        "start = time.time()\n",
        "corpus = list(reviews.body.apply(lambda x: re.sub('âœ… ', '', x)))\n",
        "corpus = [clean_doc(doc) for doc in corpus]\n",
        "end = time.time()\n",
        "# Save corpus\n",
        "file = np.array(corpus)\n",
        "np.save('corpus.npy',file)\n",
        "print('Cleaning takes {} sec in total.'.format(end-start))\n",
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA680XdZetGG"
      },
      "source": [
        "# # Load corpus\n",
        "# file = np.load('corpus.npy')\n",
        "# corpus = a.tolist()\n",
        "# corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiuOwSR0etGG"
      },
      "source": [
        "## Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUWOXAmCetGG"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lqQUHqkCetGG"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features=200, max_df=0.95, min_df=2)\n",
        "X = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(X.shape)\n",
        "print(tfidf_vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbu7UHUHetGH"
      },
      "source": [
        "### Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI1VTfWaetGH"
      },
      "source": [
        "# word2vec = Word2Vec(corpus, size=200,  window=5,  min_count=5,  negative=3, sample=0.001, hs=1, workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOYNK9-XetGH"
      },
      "source": [
        "## Topic extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3skvDMaj3hU"
      },
      "source": [
        "### NMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihXU-7FTetGH"
      },
      "source": [
        "# NMF decomposition\n",
        "# n_components: Number of topics\n",
        "nmf = NMF(n_components = 8, init='nndsvd', max_iter=300, alpha=0.1, l1_ratio=0.5)\n",
        "# Get the W matrix\n",
        "W = nmf.fit_transform(X)\n",
        "# Get the H matrix\n",
        "H = nmf.components_\n",
        "\n",
        "# To print the 5 top words of each of the 8 topics\n",
        "def print_top_words(H, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(H):\n",
        "        print(\"Topic #%d:\" % (topic_idx+1))\n",
        "        print(\", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]]))\n",
        "\n",
        "print_top_words(H, tfidf_vectorizer.get_feature_names(), n_top_words = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eB6avsISetGI"
      },
      "source": [
        "### LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "92uOPjxzetGI"
      },
      "source": [
        "# LDA model\n",
        "start = time.time()\n",
        "lda = LDA(n_components=8, n_jobs=-1, verbose=5)\n",
        "# Train the model\n",
        "Y = lda.fit_transform(X)\n",
        "end = time.time()\n",
        "print('LDA training takes {} sec.'.format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDF1H2nTetGI"
      },
      "source": [
        "# Make dataframe\n",
        "topic_names = [\"Topic\" + str(i) for i in range(1, lda.n_components + 1)]\n",
        "df_document_topic = pd.DataFrame(np.round(Y, 2), columns = topic_names)\n",
        "# Get dominant topic for each document\n",
        "dominant_topic = (np.argmax(df_document_topic.values, axis=1)+1)\n",
        "df_document_topic['Dominant_topic'] = dominant_topic\n",
        "df_document_topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Th4y2ESetGJ"
      },
      "source": [
        "# Make dataframe\n",
        "docnames = ['Doc' + str(i) for i in range(len(corpus))]\n",
        "df_document_topic = pd.DataFrame(np.round(Y, 2), columns=topic_names, index=docnames)\n",
        "\n",
        "# Get dominant topic for each document\n",
        "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
        "df_document_topic['dominant_topic'] = dominant_topic\n",
        "\n",
        "# Topic-Keyword Matrix\n",
        "df_topic_keywords = pd.DataFrame(lda.components_)\n",
        "\n",
        "# Assign Column and Index\n",
        "df_topic_keywords.columns = tfidf_vectorizer.get_feature_names()\n",
        "df_topic_keywords.index = topic_names\n",
        "\n",
        "df_topic_no = pd.DataFrame(df_topic_keywords.idxmax())\n",
        "df_scores = pd.DataFrame(df_topic_keywords.max())\n",
        "\n",
        "tmp = pd.merge(df_topic_no, df_scores, left_index=True, right_index=True)\n",
        "tmp.columns = ['topic', 'relevance_score']\n",
        "\n",
        "# Show keywords related to certain topic\n",
        "all_topics = []\n",
        "for i in tmp['topic'].unique():    \n",
        "    tmp_1 = tmp.loc[tmp['topic'] == i].reset_index()\n",
        "    tmp_1 = tmp_1.sort_values('relevance_score', ascending=False).head(5)\n",
        "    \n",
        "    tmp_2 = []\n",
        "    tmp_2.append(tmp_1['topic'].unique()[0])\n",
        "    tmp_2.append(list(tmp_1['index'].unique()))\n",
        "    all_topics.append(tmp_2)\n",
        "\n",
        "all_topics = pd.DataFrame(all_topics, columns=['Dominant_topic', 'keywords'])\n",
        "all_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dznPXhUhetGJ"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynt1NPcvj-x-"
      },
      "source": [
        "### t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt1PdPwietGJ"
      },
      "source": [
        "# # Calculate the clustering target\n",
        "# start = time.time()\n",
        "# target = np.array([np.argmax(Y[i]) for i in range(len(Y))])\n",
        "# tsne = TSNE()\n",
        "# trans = tsne.fit_transform(Y)\n",
        "# data = pd.DataFrame(np.concatenate((trans, target.reshape(-1,1)), axis=1), columns=['F1', 'F2', 'labels'])\n",
        "# data['labels'] = data.labels.apply(int)\n",
        "# plt.figure(figsize=(10, 10))\n",
        "# sns.scatterplot(data=data, x='F1', y='F2', hue='labels', legend=True)\n",
        "# plt.title('t-SNE Dimension Reduction of Topic Clustering')\n",
        "# plt.show()\n",
        "# plt.savefig('tsne.png')\n",
        "# end = time.time()\n",
        "# print('t-SNE takes {} sec.'.format(end-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueEQyvX_etGJ"
      },
      "source": [
        "### WordCloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46iHtkr3etGK"
      },
      "source": [
        "plt.figure(figsize=(15,5))\n",
        "for i in range(8):\n",
        "    plt.subplot(2, 4, i+1)\n",
        "    temp = all_topics[all_topics.Dominant_topic=='Topic'+str(i+1)].keywords.iloc[0]\n",
        "    wc = ', '.join([w for w in temp])\n",
        "    cloud = WordCloud(width=800, height=600, mode='RGBA', background_color=None).generate(wc)\n",
        "    plt.imshow(cloud, interpolation='bilinear')\n",
        "    plt.title('Topic '+ str(i+1))\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()\n",
        "plt.savefig('wordcloud1.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJuwObwGetGK"
      },
      "source": [
        "# Aspect-based Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIeW_Q7uetGK"
      },
      "source": [
        "# Only the first 5 keywords will be used\n",
        "all_topics.keywords.apply(lambda x: x[:2])\n",
        "all_topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQk_oyGEetGK"
      },
      "source": [
        "## Sentiment analysis function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5aAoM2getGK"
      },
      "source": [
        "# Load pretrained ABSA model\n",
        "nlp = absa.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wWJSIaoetGL"
      },
      "source": [
        "# Target text\n",
        "text = reviews.body.iloc[-1000]\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugESgypnetGL"
      },
      "source": [
        "# Aspects of a topic\n",
        "aspects = all_topics[all_topics.Dominant_topic=='Topic'+str(7)].keywords.iloc[0]\n",
        "aspects"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vchphwretGL"
      },
      "source": [
        "# Sentiment Analysis\n",
        "asp = list(nlp(text, aspects))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6i_qjabetGL"
      },
      "source": [
        "# Show analysis results\n",
        "for i, a in enumerate(asp):\n",
        "    print(aspects[i], '\\t', a.sentiment, '\\t', a.scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07wiguTuetGL"
      },
      "source": [
        "# Overall score on this topic\n",
        "neu, neg, pos = .0, .0, .0\n",
        "for a in asp:\n",
        "    neu += a.scores[0]\n",
        "    neg += a.scores[1]\n",
        "    pos += a.scores[2]\n",
        "neu/len(asp), neg/len(asp), pos/len(asp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5XppAeGetGM"
      },
      "source": [
        "## Reviews comprehension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYTGHCLKetGM"
      },
      "source": [
        "# Reviews Comprehension Function\n",
        "def ReviewsComprehension(docs, topics, path='result.csv'):\n",
        "    # Number of documents and topics\n",
        "    n_topics = len(topics)\n",
        "    n_docs = len(docs)\n",
        "    \n",
        "    # Make DataFrame\n",
        "    columns = ['Topic'+str(i+1) for i in range(n_topics)] + ['result']\n",
        "    index = range(n_docs)\n",
        "    result = pd.DataFrame(columns=columns, index=index)\n",
        "    \n",
        "    # Sentiment analysis\n",
        "    for i in tqdm(range(n_docs)):\n",
        "        text = docs[i]\n",
        "        for j in range(n_topics):\n",
        "            aspects = topics[topics.Dominant_topic=='Topic'+str(j+1)].keywords.iloc[0]\n",
        "            asp = list(nlp(text, aspects))\n",
        "            neu, neg, pos = .0, .0, .0\n",
        "            for a in asp:\n",
        "                neu += a.scores[0]\n",
        "                neg += a.scores[1]\n",
        "                pos += a.scores[2]\n",
        "            neu, neg, pos = neu/len(asp), neg/len(asp), pos/len(asp)\n",
        "            if neu > neg and neu > pos:\n",
        "                result.iloc[i,j] = 0\n",
        "            if neg > neu and neg > pos:\n",
        "                result.iloc[i, j] = -1\n",
        "            if pos > neu and pos > neg:\n",
        "                result.iloc[i, j] = 1\n",
        "        # Calculate the overall review        \n",
        "        result.iloc[i, -1] = result.iloc[i, :-1].sum()\n",
        "        # Save the result every 50 documents\n",
        "        if (i+1) % 10 == 0:\n",
        "            result.to_csv(path)\n",
        "            \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fvKKp7zqEEz"
      },
      "source": [
        "# Some reviews are too long for our model to digest\n",
        "data = list(reviews.body)\n",
        "len_ = []\n",
        "for doc in data:\n",
        "    len_.append(len(doc.split()))\n",
        "\n",
        "sns.set_style('dark')\n",
        "plt.hist(len_)\n",
        "plt.title('Some reviews are too long (>512 tokens)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X7_bmfyuCLK"
      },
      "source": [
        "# Split some lenthy texts to fit into the model\n",
        "data_ = []\n",
        "for doc in data:\n",
        "    t = doc.split()\n",
        "    if len(t) > 300 and len(t) < 600:\n",
        "        data_.append(' '.join(t[:300]))\n",
        "        data_.append(' '.join(t[300:]))\n",
        "    if len(t) > 600 and len(t) < 900:\n",
        "        data_.append(' '.join(t[:300]))\n",
        "        data_.append(' '.join(t[300:600]))\n",
        "        data_.append(' '.join(t[600:]))\n",
        "    if len(t) > 900 and len(t) < 1200:\n",
        "        data_.append(' '.join(t[:300]))\n",
        "        data_.append(' '.join(t[300:600]))\n",
        "        data_.append(' '.join(t[600:900]))\n",
        "        data_.append(' '.join(t[900:]))\n",
        "    if len(t) > 1200 and len(t) < 1500:\n",
        "        data_.append(' '.join(t[:300]))\n",
        "        data_.append(' '.join(t[300:600]))\n",
        "        data_.append(' '.join(t[600:900]))\n",
        "        data_.append(' '.join(t[900:1200]))\n",
        "        data_.append(' '.join(t[1200:]))\n",
        "    if len(t) > 1500:\n",
        "        data_.append(' '.join(t[:300]))\n",
        "        data_.append(' '.join(t[300:600]))\n",
        "        data_.append(' '.join(t[600:900]))\n",
        "        data_.append(' '.join(t[900:1200]))\n",
        "        data_.append(' '.join(t[1200:1500]))\n",
        "        data_.append(' '.join(t[1500:]))\n",
        "    else:\n",
        "        data_.append(doc)\n",
        "# for doc in data:\n",
        "#     t = doc.split()\n",
        "#     if len(t) < 300:\n",
        "#         data_.append(' '.join(t))\n",
        "len(data_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ouA9jISetGM"
      },
      "source": [
        "# Comprehense reviews\n",
        "ReviewsComprehension(data_, all_topics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7n0EODzetGN"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJHP3xXXetGN"
      },
      "source": [
        "# Load the data\n",
        "result = pd.read_csv('result.csv')\n",
        "result.dropna(inplace=True)\n",
        "result.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNa_uiEjetGN"
      },
      "source": [
        "# Support function for plotting\n",
        "def stat(result, n_topics):\n",
        "    topics = ['Topic'+ str(i+1) for i in range(n_topics)]\n",
        "    df = pd.DataFrame(columns=['Topic', 'Sentiment', 'Counts'], index=range(3*n_topics))\n",
        "    i = 0\n",
        "    for t in topics:\n",
        "        temp = result[t].value_counts()\n",
        "        for idx in temp.index.tolist():\n",
        "            if idx == -1.0:\n",
        "                df.iloc[i, 0] = t\n",
        "                df.iloc[i, 1] = 'Negative'\n",
        "                df.iloc[i, 2] = temp[idx]\n",
        "                i += 1\n",
        "            if idx == 1.0:\n",
        "                df.iloc[i, 0] = t\n",
        "                df.iloc[i, 1] = 'Postive'\n",
        "                df.iloc[i, 2] = temp[idx]\n",
        "                i += 1\n",
        "            if idx == 0.0:\n",
        "                df.iloc[i, 0] = t\n",
        "                df.iloc[i, 1] = 'Neutral'\n",
        "                df.iloc[i, 2] = temp[idx]\n",
        "                i += 1\n",
        "    df.dropna(inplace=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofjgeHkgetGN"
      },
      "source": [
        "# Visualize the results\n",
        "temp = stat(result, 8)\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(data=temp, x='Topic', y='Counts', hue='Sentiment')\n",
        "plt.title('Aspect-based Topic Sentiment Analysis')\n",
        "plt.savefig('sentiment.png', bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-mkEC-c_EMc"
      },
      "source": [
        "## Further observation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQnQNXC7_L44"
      },
      "source": [
        "# Different class information\n",
        "reviews.seat_type.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEd5ty7CBvuK"
      },
      "source": [
        "# Extract the most recent reviews for different class\n",
        "economy_reviews = list(reviews[reviews.seat_type == 'Economy Class'].body[:200])\n",
        "business_reviews = list(reviews[reviews.seat_type == 'Business Class'].body[:200])\n",
        "premium_reviews = list(reviews[reviews.seat_type == 'Premium Economy'].body[:200])\n",
        "first_reviews = list(reviews[reviews.seat_type == 'First Class'].body[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDgbIaXRGWO2"
      },
      "source": [
        "# Deal with the length issues\n",
        "economy = []\n",
        "business = []\n",
        "premium = []\n",
        "first = []\n",
        "\n",
        "for doc in economy_reviews:\n",
        "    t = doc.split()\n",
        "    if len(t) < 300:\n",
        "        economy.append(' '.join(t))\n",
        "\n",
        "for doc in business_reviews:\n",
        "    t = doc.split()\n",
        "    if len(t) < 300:\n",
        "        business.append(' '.join(t))\n",
        "\n",
        "for doc in premium_reviews:\n",
        "    t = doc.split()\n",
        "    if len(t) < 300:\n",
        "        premium.append(' '.join(t))\n",
        "\n",
        "for doc in first_reviews:\n",
        "    t = doc.split()\n",
        "    if len(t) < 300:\n",
        "        first.append(' '.join(t))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNNJQBrTIVuR"
      },
      "source": [
        "# Sentiment analysis\n",
        "temp1 = ReviewsComprehension(economy, all_topics, 'economy.csv')\n",
        "temp2 = ReviewsComprehension(business, all_topics, 'business.csv')\n",
        "temp3 = ReviewsComprehension(premium, all_topics, 'premium.csv')\n",
        "temp4 = ReviewsComprehension(first, all_topics, 'first.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rchUvaIJOih"
      },
      "source": [
        "## Visualize the results\n",
        "# Economy Class\n",
        "result = pd.read_csv('economy.csv')\n",
        "result.dropna(inplace=True)\n",
        "temp = stat(result, 8)\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(data=temp, x='Topic', y='Counts', hue='Sentiment')\n",
        "plt.title('Economy Class Sentiment Analysis')\n",
        "plt.savefig('economy.png', bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp1zQKUeJyjN"
      },
      "source": [
        "# Business Class\n",
        "result = pd.read_csv('business.csv')\n",
        "result.dropna(inplace=True)\n",
        "temp = stat(result, 8)\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(data=temp, x='Topic', y='Counts', hue='Sentiment')\n",
        "plt.title('Business Class Sentiment Analysis')\n",
        "plt.savefig('business.png', bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yku7be-8J5ds"
      },
      "source": [
        "# Premium Economy Class\n",
        "result = pd.read_csv('premium.csv')\n",
        "result.dropna(inplace=True)\n",
        "temp = stat(result, 8)\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(data=temp, x='Topic', y='Counts', hue='Sentiment')\n",
        "plt.title('Premium Economy Sentiment Analysis')\n",
        "plt.savefig('premium.png', bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIoIL7mCKDr8"
      },
      "source": [
        "# First Class\n",
        "result = pd.read_csv('first.csv')\n",
        "result.dropna(inplace=True)\n",
        "temp = stat(result, 8)\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.barplot(data=temp, x='Topic', y='Counts', hue='Sentiment')\n",
        "plt.title('First Class Sentiment Analysis')\n",
        "plt.savefig('first.png', bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}